<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>JUST LAD</title>
  
  <subtitle>记载互联网感悟</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-10-12T11:30:15.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>吴三水</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>“测试文章，仅展示作用”</title>
    <link href="http://yoursite.com/2017/10/12/%E2%80%9C%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0%EF%BC%8C%E4%BB%85%E5%B1%95%E7%A4%BA%E4%BD%9C%E7%94%A8%E2%80%9D/"/>
    <id>http://yoursite.com/2017/10/12/“测试文章，仅展示作用”/</id>
    <published>2017-10-12T11:10:06.000Z</published>
    <updated>2017-10-12T11:30:15.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>哥伦比亚教授周以真：人工智能恐慌以及大数据威胁反思</p><p>使用不负责任的数据导致的偏见算法和严重后果，到底应该由谁负责？个人和阿里巴巴这样的企业应该用什么样的态度对待数据？以及我们该如何用科技本身保证科技的公平透明？<br>大数据文摘记者：魏子敏</p></blockquote><a id="more"></a> <p>大数据和人工智能正无可置疑地为全行业和我们的生活带来了翻天覆地的变化，在10月11日2017杭州·云栖大会的主论坛上，不同于全场众多追捧褒扬之声，来自哥伦比亚大学的周以真教授则针对大数据和人工智能可能带来的威胁进行了反思。她提出，数据会给我们带来很多好处，但是如果不负责任滥用数据和算法，会带来可怕的结果。</p><p>“我要给大家敲一个警钟，我们在使用数据过程当中不负责任的话，会有什么样的后果。” 她用FATES（命运）这个比喻的缩写来讲述了怎样有责任的使用数据，F是代表公平，A是可靠，T是透明，E是有道德，S是代表安全。演讲中周以真教授同时提出了一些引人思考的问题：使用不负责任的数据导致的偏见算法和严重后果，到底应该由谁负责？个人和阿里巴巴这样的企业应该用什么样的态度对待数据？以及我们该如何用科技本身保证科技的公平透明？</p><p>周以真现任哥伦比亚大学数据科学研究院主任，定义了计算思维。</p><p>哥伦比亚教授周以真：人工智能恐慌以及大数据威胁反思<br>哥伦比亚教授周以真在2017杭州云栖大会主论坛的演讲《Data for Good：Scary AI and Other Dangers with Big Data（大数据的好处：可怕的人工智能以及大数据的威胁）》</p><p>以下为周以真相关演讲的速记，在不改变原意的情况下有删改。</p><p>数据能够带来什么样的好处，我这里分两方面来说。</p><p>第一方面，我们要有责任的使用数据。</p><p>第二方面，我们要用数据来应对社会巨大的问题，比如说能源、环境、教育、气侯变化等等这些重大的人类命题。</p><p>但是今天我只是会去谈有责任的使用数据这一个方面，因为我们所有人都是在使用数据，我们在使用数据的目的，都是为了应对社会的挑战。我演讲的目的，给大家敲一个警钟，我们在使用数据过程当中不负责任的话，会有什么样的后果。</p><p>我想用FATES（命运）这个比喻的缩写来去讲一下怎么有责任的使用，F是代表公平，A是可靠，T是透明，E是有道德，S是代表安全。</p><p>有偏见的数据导致的问题，谁该负责任？</p><p>首先来看一下数据，我在给大家分享之前，先非常简单的讲一下典型的算法和模式，我们在用大数据为原料，进行大数据计算的时候，涉及到的一些算法和模型。我们知道机器学习和形成一种模型，有这个模型，我可以再输入新的数据，这个新的数据，经过这个模型之后，有可能产生新的结果，之后我们可以来判断和预计，这个用户有可能采购哪些商品。</p><p>我们知道数据和算法都可能是有偏见的，那么如果数据和算法是有偏见的话，那么我们的这个模型可能也会有偏见，那么我们的结果也会有偏见的。让我们来看一个实例，那么这是几年前我们看到两个小偷，在美国的法官用了这种算法来决定判断量刑过程当中是否恰当，我们看到这些自主研发的算法，在法官当中广为流行的算法，用于帮助法官去判断这些量刑。</p><p>我发现他们对于黑人和白人量刑的结果是不同的，其实结果还不仅仅于此，我们在算法当中有一些什么样的问题，这个算法本身是有偏见的，而且哈佛大学学者研究出来，这些算法有可能是可以判断的，又可能是错误的，但不可能两者兼具，实际上是不可能去判断这个风险的分数。</p><p>第二个例子，这是我的这些同事做的项目，我的这些同事研究了在Google上的广告，他们发现这些高薪的工作机会更多的会向男性网民展现，女性网民看到这些高薪的招聘广告机会会比较少，我们觉得这是不公平的。现在我们就要去思考，这个模型是否是公正的，这些分类是否是公正的，我们怎么样来确保，这个案例就引发了另外一个问题，那就是可靠性。</p><p>哥伦比亚教授周以真：人工智能恐慌以及大数据威胁反思<br>说到底，出现问题的时候，我们应该怪谁呢？</p><p>好像这个问题很难找出一个好的答案，但是我们要有担当，我们在IT界，我们是发明这些算法的人，我们是使用数据的人，我们是产生和收集这些数据的人，并且生成这些结果的人，我们要有担当。</p><p>如果你是一家企业，那么如果你是一个有责任的企业，你应该做什么，你首先可以把政策进行公布，你的隐私政策进行公布，而且你要遵守这些政策，如果有人违规的话，违反了这个政策，你就要去修补你的这个漏洞。</p><p>我和的同事在微软研究院所做的，我们会看一下在人们遵守这个公共政策的规模和程度是如何。这涉及到我们编程的语言，我们做成数据地图，这个数据地图每天晚上在微软进行运行，帮我们找到我们政策上的漏洞，所以自动化可以在这方面帮助我们，让我们负起责任来，让我们对于我们发布的政策负起责任来。</p><p>152层的DNN如何保障其透明度？</p><p>第三个关键词是透明度。</p><p>透明度现在是一个很大的问题，特别是我们涉及到这些深层的神经网络的时候，我们是否应该对这个结果予以信任，我们为什么要对这个结果予以信任呢，我们都不知道怎么样来运作的，从这个科学的角度来说，我们其实并不了解他们是如何来工作的，那这样的话，就会引起一些问题。</p><p>在给大家举例子之前，首先给大家介绍一下，我们最大的DNN（音），这是152层的DNN，那么它是获得了2015年的Image.net的竞赛奖项，这里面我们可以看到这里的DNN一共有152层，大家问为什么是要152层，事实上我们不知道为什么是152层，结果就是如此，对于科学家来讲，我们不仅仅满足于这个答案，我们看这个DNN在什么情况下会出错。</p><p>哥伦比亚教授周以真：人工智能恐慌以及大数据威胁反思<br>这个例子看出来，我们为什么使用这个DNN的时候，做图象识别的时候，要小心，这是一段视频，在这个视频当中，我们可以看到，我们在驾驶车辆，我们开车的时候，可以看到有一个车速限速度的标志，在右侧可以看到，在右边是停止的Stop的牌子，在左边是涂鸦的限速45英里的牌子，这个DNN识别到在右侧Stop的图像。它认为有了这个涂鸦的限速牌，不认为这是一个限速牌，在开车不到一秒的时间里面，我们可以看到后面驾驶的车辆，开得很近的时候，发现涂鸦的这个标牌也是Stop的标牌，但是看见的时候已经太迟了，他觉得这个时候要刹车已经来不及了，这时候就有可能发生撞车的事故。</p><p>现在不光是熊猫、猴子，还有刚才我们所提的这样一些例子，这个就是大家可以看到的，如果我们对于这个DNN怎样工作的原理不清楚的话，就会潜在的造成一些威胁。</p><p>我们再来看一个例子。我们可以看到奥巴马在同样一个音轨，同样的话，用四种语音语段发出来，这是一个Youtube上面比较好玩的事情。对于这样一个音频流，你可以知道任何人都可以模拟任何人的发音，这样就会产生威胁。</p><p>阿里巴巴这样的大公司应该怎么做？</p><p>这不单会产生技术问题，也不是写论文的问题，应该说这是一个实实在在的，对于大公司，比方说像阿里巴巴这样的大公司，正在努力致力于研究解决的这样一些问题。欧盟也有这样的政策，2018年所有大的公司，都要遵守这样的一个有关于数据方面的问题的法规章程，不然的话，你就会被罚款或者说有4%这样的营业收入就要来交营业罚款。</p><p>这里有四个标准，一个是可访问的权利，一个是可忘却的权力，一个是数据的可携带性，还有可解释的权利，2017年到2018年之间，科学家正在致力于了解深度学习到底是怎么样来进行工作的，要能够解释得清，这是一个伦理的问题。</p><p>这里面我们可以看到，这里是一个列车的问题，我们可以看到这里面一辆列车开过来，扳这里有一个选择，到底是通过扳道，是往上面的通道走还是往下面的通道走，下面可能是小孩子或者说肥胖的人，不管是把道路往哪个方向搬，这都会牵涉到伦理方面的难题。那么现在有了我们这个自动驾驶车，必须要做这样的决定，比方说在碰到类似情况的时候，这个车应该做什么样的决断。比方说在右边有一个行人，但是这个人比方说在人行道上面也有其他的人，这个车躲避的话，到底是躲避谁，撞上什么，这是很难下的决定。</p><p>这是一个假新闻的问题，那么假新闻现在也在美国到处肆虐，这里应该说假新闻泛滥，造成了很多的问题，我们大家看到微软有这样一个例子，一个聊天机器人，叫做小兵，这个聊天机器人是如此的流行，以至于在美国，我们有一点嫉妒，你们中国有这样的很好的聊天机器人，在美国还没有这么好的聊天机器人，去年微软也有了这么一个聊天机器人。</p><p>我们在24小时之内，不得不把这个聊天机器人关闭了，为什么呢？因为我们看到由于互联网之间有一些阴暗面的存在，很快我们发现聊天机器人被诱导，引导说一些很不好听的话题，这里面我们才认识到互联网，这里面也有一些快速传播的不良信息，我们要非常重视伦理道德，我们在设计的时候就要注意，而不是在运用的时候。</p><p>还有一个例子是关于安全和保密的事情，例如天猫精灵，在你家或者车里，很容易被黑客侵入，所以物联网这样的平台，如果说连到互联网这样任何的物品，很容易被坏人所侵入，这样就会造成一些影响。</p><p>如何用科技保证科技本身的公平透明？</p><p>回过头来再看一下缩写拼出的词，FATEC代表公平、透明等等，在这方面，科技能够做哪些工作呢？</p><p>我们可以看到刚才所说的，应该要产出各种可能性，有各种各样的模式模板，所以我们要让第三方别人能够来检查我们这样的一些产品，同样的道理，比如说给他们提供这样一些资料和数据，我和我的两个同事也写过一篇论文，大家有兴趣的话，也可以阅读一下，比方说你的数据谁在掌握，我们现在有很多这样的科技公司，可以看到这样一些科技公司都是尽量在确保想要把人工智能、数据往好的方面运用。</p><p>比如说亚马逊、深度思考，包括苹果、IBM、Google、脸书等等机构，现在都有更多的机构和个人，都加入进来。人工智能能够造福人类，但是我们在科技界应该承担这样的责任，能够确保往好的方面来发展，我们现在也有一些新的问题。</p><p>我们现在已经对机器人有相关的立法，对人工智能是否也要立法，那么人工智能是不是也要进行很好的管制，包括这样一些平台，包括一些使用，人工智能的这样一些管道，是否也应该进行管制呢，如果要管制的话，是由谁来管制呢，我们是否要有一个消费者保护，有一个保险，还有比方说一些经济上面的奖励，以避免这样一些人工智能不良的应用。</p><p>包括我们所有的产品是否需要有一个授权许可，公司是否也需要有这样一个委员会专门来进行检查和审核。所以我们对于这个数据有一个负责任的态度来使用，才能够物尽其用，谢谢。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;哥伦比亚教授周以真：人工智能恐慌以及大数据威胁反思&lt;/p&gt;
&lt;p&gt;使用不负责任的数据导致的偏见算法和严重后果，到底应该由谁负责？个人和阿里巴巴这样的企业应该用什么样的态度对待数据？以及我们该如何用科技本身保证科技的公平透明？&lt;br&gt;大数据文摘记者：魏子敏&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="互联网资讯" scheme="http://yoursite.com/categories/%E4%BA%92%E8%81%94%E7%BD%91%E8%B5%84%E8%AE%AF/"/>
    
    
      <category term="资讯" scheme="http://yoursite.com/tags/%E8%B5%84%E8%AE%AF/"/>
    
  </entry>
  
</feed>
